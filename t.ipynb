{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "# from datadings.tools import yield_threaded\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Harmony\n",
    "\n",
    "from models.clip import CLIP_VITL16\n",
    "from models.dinov2 import vit_large\n",
    "import argparse\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(arch='vit_small', batch_size_per_gpu=1, clip_grad=3.0, data='CC3M:/mnt/d/data/CC3M/cc3m/', dist_url='env://', drop_path_rate=0.1, epochs=100, freeze_last_layer=1, global_crops_scale=(0.4, 1.0), gpu=0, local_crops_number=8, local_crops_scale=(0.05, 0.4), local_rank=0, lr=0.0005, min_lr=1e-06, momentum_teacher=0.996, norm_last_layer=True, num_workers=10, objective='dino', optimizer='adamw', out_dim=65536, output_dir='/mnt/c/Users/Moham/Desktop/KAUST/results', patch_size=16, rank=0, saveckp_freq=20, seed=0, teacher_temp=0.04, use_bn_in_head=False, use_fp16=True, warmup_epochs=10, warmup_teacher_temp=0.04, warmup_teacher_temp_epochs=0, weight_decay=0.04, weight_decay_end=0.4, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.init_distributed_mode(args)\n",
    "utils.fix_random_seeds(args.seed)\n",
    "\n",
    "model = Harmony(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from vision_transformer import Block\n",
    "\n",
    "from utils import get_2d_sincos_pos_embed\n",
    "\n",
    "class GenerativePath(nn.Module):\n",
    "    def __init__(self, image_encoder, patch_size=16, in_chans=3,\n",
    "                embed_dim=1024, decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                mlp_ratio=4, norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.image_encoder = image_encoder\n",
    "        self.patch_embed = image_encoder.patch_embed\n",
    "        num_patches = self.image_encoder.patch_embed.num_patches\n",
    "\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "\n",
    "    def initialize_deocder_weights(self):\n",
    "        \n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        print(self.patch_embed.patch_size)\n",
    "        p = self.patch_embed.patch_size\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.image_encoder.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.image_encoder.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.image_encoder.cls_token + self.image_encoder.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.image_encoder.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.image_encoder.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = GenerativePath(image_encoder=model.image_encoder, embed_dim=384).cuda()\n",
    "r = torch.rand([1, 3, 224, 224]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, p, m = s(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP_VITL16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [f for f in os.scandir(\"/mnt/d/data/CC3M/cc3m/\") if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_in_folders(folders):\n",
    "    images_paths = []\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            if os.path.isfile(os.path.join(folder, filename)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = save_images_in_folders(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def save_image_captions_in_folders(folders, root):\n",
    "    images_paths  = []\n",
    "    captions_path = []\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            if os.path.isfile(os.path.join(folder, filename)):\n",
    "                if \".jpg\" in filename or \".png\" in filename:\n",
    "                    images_paths.append(root + os.sep + folder.name + os.sep + filename)\n",
    "                elif \".txt\" in filename:\n",
    "                    captions_path.append(root + os.sep + folder.name + os.sep + filename)\n",
    "        break\n",
    "    return images_paths, captions_path\n",
    "\n",
    "class CC3M(torch.utils.data.Dataset):\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.folders =  [f for f in os.scandir(root) if f.is_dir()]\n",
    "        self.images, self.captions = save_image_captions_in_folders(self.folders, self.root)\n",
    "\n",
    "        assert len(self.captions) == len(self.images)\n",
    "        print(\"Number of images loaded in CC3M are: \", {self.__len__()})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def get_image_caption_pair(self, idx):\n",
    "        image = Image.open(self.images[idx])\n",
    "        \n",
    "        caption_file = open(self.captions[idx])\n",
    "        caption = caption_file.read()\n",
    "        caption_file.close()\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, caption = self.get_image_caption_pair(idx)\n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CC3M(root=\"/mnt/d/data/CC3M/cc3m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, t = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Open the tar file\n",
    "tar = tarfile.open(r'D:\\data\\CC3M\\cc3m\\00000.tar')\n",
    "\n",
    "# Loop over each member\n",
    "for member in tar.getmembers():\n",
    "    print(member)\n",
    "    # Extract each file as a file object\n",
    "    f = tar.extractfile(member)\n",
    "    if f is not None:\n",
    "        # Read the contents\n",
    "        content = f.read()\n",
    "\n",
    "# Close the tar file\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
