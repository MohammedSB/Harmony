{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Moham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "# from datadings.tools import yield_threaded\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Harmony.models import Harmony\n",
    "\n",
    "# from Harmony.models.clip import CLIP_VITL16\n",
    "# from Harmony.models.dinov2 import vit_large\n",
    "import argparse\n",
    "\n",
    "from Harmony import utils\n",
    "import Harmony.models.vision_transformer as vits\n",
    "\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "zip_directory = \"D:/data/YFCC15M/\"\n",
    "zips = [zip for zip in os.listdir(\"D:\\data\\YFCC15M\") if zip[-4:] == \".zip\"]\n",
    "# zips = zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4098/4098 [02:20<00:00, 29.22it/s]\n"
     ]
    }
   ],
   "source": [
    "image_paths = []\n",
    "\n",
    "def list_images_in_zip(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # List all contents and filter for image files\n",
    "        image_files = [item for item in zip_ref.namelist() if item.lower().endswith(('.jpg'))]\n",
    "    return image_files\n",
    "\n",
    "for zip_file in tqdm(zips):\n",
    "    zip_path = os.path.join(zip_directory, zip_file)\n",
    "    image_paths.extend(list_images_in_zip(zip_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14825236"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14825236"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_unzipped_size(zip_path):\n",
    "    total_size = 0\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        for info in zip_ref.infolist():\n",
    "            total_size += info.file_size  # Use `file_size` for uncompressed size\n",
    "    return total_size\n",
    "\n",
    "all_size = 0\n",
    "\n",
    "for zip_file in zips:\n",
    "    zip_path = os.path.join(zip_directory, zip_file)\n",
    "    estimated_size = estimate_unzipped_size(zip_path)\n",
    "    all_size += estimated_size\n",
    "    # print(f\"Estimated size for {zip_file} when unzipped: {estimated_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1913923736556"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_aggregated_size_gb = all_size / (1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1782.4803819470108"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_aggregated_size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations, permutations\n",
    "\n",
    "def power_set_permutations(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(\"_\".join(t) for t in chain.from_iterable(permutations(t) for t in chain.from_iterable(combinations(s, r) for r in range(len(s)+1))))\n",
    "\n",
    "# Test the function\n",
    "print(power_set_permutations(['ibot', 'dino', 'mae']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_ = torch.load(\"ibot_t.pth\")\n",
    "ht  = torch.load(\"harmony_t.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ = torch.load(\"ibot_s.pth\")\n",
    "hs  = torch.load(\"harmony_s.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_same = True\n",
    "\n",
    "# params1 = is_\n",
    "params2 = hs\n",
    "\n",
    "params1 = it_\n",
    "# params2 = ht\n",
    "\n",
    "for key in params1:\n",
    "    if key in params2:\n",
    "        if not torch.equal(params1[key], params2[key]):\n",
    "            print(f\"Difference found in parameter: {key}\")\n",
    "            is_same = False\n",
    "    else:\n",
    "        print(f\"Parameter {key} not found in the second model.\")\n",
    "        is_same = False\n",
    "\n",
    "for key in params2:\n",
    "    if key not in params1:\n",
    "        print(f\"Parameter {key} not found in the first model.\")\n",
    "        is_same = False\n",
    "\n",
    "if is_same:\n",
    "    print(\"The parameters of the two models are exactly the same.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_.values(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht.values() == it_.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(arch='vit_small', batch_size_per_gpu=1, clip_grad=3.0, data='CC3M:/mnt/d/data/CC3M/cc3m/', dist_url='env://', drop_path_rate=0.1, epochs=100, freeze_last_layer=1, global_crops_scale=(0.4, 1.0), gpu=0, local_crops_number=8, local_crops_scale=(0.05, 0.4), local_rank=0, lr=0.0005, min_lr=1e-06, momentum_teacher=0.996, norm_last_layer=True, num_workers=10, objective='dino', optimizer='adamw', out_dim=65536, output_dir='/mnt/c/Users/Moham/Desktop/KAUST/results', patch_size=16, rank=0, saveckp_freq=20, seed=0, teacher_temp=0.04, use_bn_in_head=False, use_fp16=True, warmup_epochs=10, warmup_teacher_temp=0.04, warmup_teacher_temp_epochs=0, weight_decay=0.04, weight_decay_end=0.4, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(r\"/mnt/c/Users/Moham/Desktop/KAUST/results/checkpoint0000.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.init_distributed_mode(args)\n",
    "# utils.fix_random_seeds(args.seed)\n",
    "\n",
    "# model = Harmony(args=args)\n",
    "\n",
    "model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(checkpoint['main_vit'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_key = \"main_vit\"\n",
    "state_dict = torch.load(r\"/mnt/c/Users/Moham/Desktop/KAUST/results/checkpoint0000.pth\", map_location=\"cpu\")\n",
    "\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "# state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# # remove `backbone.` prefix induced by multicrop wrapper\n",
    "# state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "msg = model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = \"/mnt/c/Users/Moham/Desktop/KAUST/results/checkpoint.pth\"\n",
    "checkpoint_key = \"model\"\n",
    "state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.discriminative_path.teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.discriminative_path.teacher.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.discriminative_path.teacher.backbone.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "# train_transform = pth_transforms.Compose([\n",
    "#     pth_transforms.RandomResizedCrop(224),\n",
    "#     pth_transforms.RandomHorizontalFlip(),\n",
    "#     pth_transforms.ToTensor(),\n",
    "#     pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "# ])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(\"/mnt/e/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train\")\n",
    "\n",
    "data = torch.utils.data.DataLoader(train_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dataset[445245])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset[1277363]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for i in range(data.__len__()):\n",
    "    r = data.dataset[i]\n",
    "    print(\"class:\", r[1])\n",
    "    print(\"image:\", r[0])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.realpath(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageNet(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split=\"train\", transform=None, **kwargs):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.data = datasets.ImageFolder(root, transform=transform)\n",
    "        elif split == \"val\":\n",
    "            self.images =  os.listdir(self.root)\n",
    "            self.image_paths = [os.path.join(self.root, image) for image in self.images]\n",
    "            self.labels = pd.read_csv(\"./Harmony/data/meta/imagenet_val_labels.csv\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return self.data.__len__()\n",
    "        else:\n",
    "            return len(self.images)\n",
    "\n",
    "    def get_image_target(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image_name = self.images[idx].split('.')[0]\n",
    "        label = self.labels[self.labels['ImageId'] == image_name]['PredictionString'].item()\n",
    "        return image, label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            return self.data[idx][0], self.data[idx][1]\n",
    "        \n",
    "        image, target = self.get_image_target(idx)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageNet(root=\"/mnt/e/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val/\", split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in data:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from vision_transformer import Block\n",
    "\n",
    "from utils import get_2d_sincos_pos_embed\n",
    "\n",
    "class GenerativePath(nn.Module):\n",
    "    def __init__(self, image_encoder, patch_size=16, in_chans=3,\n",
    "                embed_dim=1024, decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                mlp_ratio=4, norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.image_encoder = image_encoder\n",
    "        self.patch_embed = image_encoder.patch_embed\n",
    "        num_patches = self.image_encoder.patch_embed.num_patches\n",
    "\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "\n",
    "    def initialize_deocder_weights(self):\n",
    "        \n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        print(self.patch_embed.patch_size)\n",
    "        p = self.patch_embed.patch_size\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.image_encoder.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.image_encoder.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.image_encoder.cls_token + self.image_encoder.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.image_encoder.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.image_encoder.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = GenerativePath(image_encoder=model.image_encoder, embed_dim=384).cuda()\n",
    "r = torch.rand([1, 3, 224, 224]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, p, m = s(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP_VITL16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [f for f in os.scandir(\"/mnt/d/data/CC3M/cc3m/\") if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_in_folders(folders):\n",
    "    images_paths = []\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            if os.path.isfile(os.path.join(folder, filename)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = save_images_in_folders(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from Harmony.data.utils import SimpleTokenizer\n",
    "\n",
    "def save_image_captions_from_folders(folders, root):\n",
    "    images_paths  = []\n",
    "    captions_path = []\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            if \".jpg\" in filename or \".png\" in filename:\n",
    "                images_paths.append(root + os.sep + folder.name + os.sep + filename)\n",
    "            elif \".txt\" in filename:\n",
    "                captions_path.append(root + os.sep + folder.name + os.sep + filename)\n",
    "    return images_paths, captions_path\n",
    "\n",
    "class CC3M(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, tokneizer=SimpleTokenizer(), **kwargs):\n",
    "        self.root = root\n",
    "        self.folders =  [f for f in os.scandir(root) if f.is_dir()]\n",
    "        self.images, self.captions = save_image_captions_from_folders(self.folders, self.root)\n",
    "        self.images.sort(), self.captions.sort() # sort to make sure there is correspondence\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokneizer\n",
    "\n",
    "        # self.images = self.images[:10]\n",
    "        # self.captions = self.captions[:10]\n",
    "\n",
    "        assert len(self.captions) == len(self.images)\n",
    "        print(\"Number of images loaded in CC3M are:\", {self.__len__()})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def get_image_caption_pair(self, idx):\n",
    "        print(self.images[idx])\n",
    "        print(self.captions[idx])\n",
    "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        caption_file = open(self.captions[idx])\n",
    "        caption = caption_file.read()\n",
    "        caption_file.close()\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, caption = self.get_image_caption_pair(idx)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.tokenizer:\n",
    "            caption = self.tokenizer(caption)\n",
    "    \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CC3M(root=\"D:\\data\\CC3M\\cc3m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, t = a[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Open the tar file\n",
    "tar = tarfile.open(r'D:\\data\\CC3M\\cc3m\\00000.tar')\n",
    "\n",
    "# Loop over each member\n",
    "for member in tar.getmembers():\n",
    "    print(member)\n",
    "    # Extract each file as a file object\n",
    "    f = tar.extractfile(member)\n",
    "    if f is not None:\n",
    "        # Read the contents\n",
    "        content = f.read()\n",
    "\n",
    "# Close the tar file\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
