{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Moham\\AppData\\Local\\Temp\\ipykernel_20144\\2477414774.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "c:\\Users\\Moham\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "# from datadings.tools import yield_threaded\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Harmony.models import Harmony\n",
    "\n",
    "# from Harmony.models.clip import CLIP_VITL16\n",
    "# from Harmony.models.dinov2 import vit_large\n",
    "import argparse\n",
    "\n",
    "from Harmony import utils\n",
    "import Harmony.models.vision_transformer as vits\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import gzip\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Harmony.data.utils import SimpleTokenizer\n",
    "from PIL import Image\n",
    "\n",
    "def get_files_from_root(directory):\n",
    "    f = []\n",
    "    for root, dirs, files in tqdm(os.walk(directory)):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            f.append(file_path.split(os.sep)[-1].split(\".\")[0])\n",
    "    return f\n",
    "\n",
    "class YFCC15M(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, tokneizer=SimpleTokenizer(), **kwargs):\n",
    "        self.root = root\n",
    "        \n",
    "        files = get_files_from_root(self.root + os.sep + \"images\")\n",
    "        self.df = pd.read_csv(root + os.sep + \"yfcc15m.csv\")\n",
    "        indices = self.df['1'].isin(files)\n",
    "        self.df = self.df[indices]\n",
    "        \n",
    "        self.image_captions = [tuple(x[2:]) for x in self.df.to_numpy()]\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokneizer\n",
    "        print(\"Number of images loaded in YFCC15M are:\", {self.__len__()})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_captions)\n",
    "\n",
    "    def get_image_caption_pair(self, idx):\n",
    "        item = self.image_captions[0]\n",
    "        path = item[0]\n",
    "        path = self.root + os.sep + \"images\" + os.sep + path[:3] + os.sep + path[3:6] + os.sep + path + \".jpg\"\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        caption = np.random.choice([item[1], item[2]])\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, caption = self.get_image_caption_pair(idx)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.tokenizer:\n",
    "            caption = self.tokenizer(caption)\n",
    "    \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1372291it [03:21, 6817.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images loaded in YFCC15M are: {2165959}\n"
     ]
    }
   ],
   "source": [
    "r = YFCC15M(root=\"D:\\data\\YFCC15M\\YFCC15M_extracted\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=500x375>,\n",
       " tensor([49406, 14450,  4233,  1063, 49407,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = r.df['1'].isin(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            True\n",
       "1            True\n",
       "2            True\n",
       "3            True\n",
       "4            True\n",
       "            ...  \n",
       "14829391    False\n",
       "14829392    False\n",
       "14829393    False\n",
       "14829394    False\n",
       "14829395    False\n",
       "Name: 1, Length: 14829396, dtype: bool"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = r.df[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = r.df[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>http://farm1.staticflickr.com/220/478637637_f2...</td>\n",
       "      <td>00156d23e1eaf8edaad4748a2cbd</td>\n",
       "      <td>Chilly soccer game</td>\n",
       "      <td>Dad's a good sport specatator, keeping warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://farm4.staticflickr.com/3157/2342279336_...</td>\n",
       "      <td>0013f5df6b77c1c61770a61d5d2687</td>\n",
       "      <td>Stinson Reliant Information</td>\n",
       "      <td>Information panel for the Stinson Reliant mail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://farm3.staticflickr.com/2010/2365135885_...</td>\n",
       "      <td>001dd37fa9ec4ec6c502e893b8476</td>\n",
       "      <td>Nightfall Between Bridges, Brooklyn, NY</td>\n",
       "      <td>A panoramic shot of Manhattan between the Broo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://farm3.staticflickr.com/2389/2515710583_...</td>\n",
       "      <td>0019dcc1de5e66ddabec2c2790fb6f</td>\n",
       "      <td>First amphibious guest of the season!</td>\n",
       "      <td>Found this sweetie lounging on a lily pad in m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://farm4.staticflickr.com/3178/2635470994_...</td>\n",
       "      <td>0016c8879d709d7f9eae5ddff5ebcd</td>\n",
       "      <td>Birthday Weekend</td>\n",
       "      <td>One thing I learned this weekend was to never ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187129</th>\n",
       "      <td>2187129</td>\n",
       "      <td>http://farm3.staticflickr.com/2916/13923343541...</td>\n",
       "      <td>32a0ea699c2897492580f8e71256d19e</td>\n",
       "      <td>Salt Lake County, Utah, Wasatch Hollow, Salt L...</td>\n",
       "      <td>Check out the related YouTube video here. - \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187134</th>\n",
       "      <td>2187134</td>\n",
       "      <td>http://farm8.staticflickr.com/7003/13937328663...</td>\n",
       "      <td>32a0b0d2f3ba879a9dc483781b152b28</td>\n",
       "      <td>Barclays Center Arena - 20140419_1220</td>\n",
       "      <td>Barclays Center Arena\\nAtlantic Yards\\n6th and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187136</th>\n",
       "      <td>2187136</td>\n",
       "      <td>http://farm8.staticflickr.com/7348/13954790274...</td>\n",
       "      <td>32a11e7ba9e6dd474755aadbe92bd942</td>\n",
       "      <td>Capitol Hill, District of Columbia, Washington...</td>\n",
       "      <td>Check out the related YouTube video here. - \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187145</th>\n",
       "      <td>2187145</td>\n",
       "      <td>http://farm3.staticflickr.com/2919/14009565002...</td>\n",
       "      <td>32a266972a14b2f4a1a39721d9ab36da</td>\n",
       "      <td>Bermuda</td>\n",
       "      <td>Honeymoon in Bermuda at the Sonesta Beach Reso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187146</th>\n",
       "      <td>2187146</td>\n",
       "      <td>http://farm3.staticflickr.com/2909/14010305852...</td>\n",
       "      <td>32a3232e355d409cc7f9b0d2b7638326</td>\n",
       "      <td>Hil, her Mom...</td>\n",
       "      <td>...and Hil's fab shoes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2165959 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0                                                  0  \\\n",
       "0                 0  http://farm1.staticflickr.com/220/478637637_f2...   \n",
       "1                 1  http://farm4.staticflickr.com/3157/2342279336_...   \n",
       "2                 2  http://farm3.staticflickr.com/2010/2365135885_...   \n",
       "3                 3  http://farm3.staticflickr.com/2389/2515710583_...   \n",
       "4                 4  http://farm4.staticflickr.com/3178/2635470994_...   \n",
       "...             ...                                                ...   \n",
       "2187129     2187129  http://farm3.staticflickr.com/2916/13923343541...   \n",
       "2187134     2187134  http://farm8.staticflickr.com/7003/13937328663...   \n",
       "2187136     2187136  http://farm8.staticflickr.com/7348/13954790274...   \n",
       "2187145     2187145  http://farm3.staticflickr.com/2919/14009565002...   \n",
       "2187146     2187146  http://farm3.staticflickr.com/2909/14010305852...   \n",
       "\n",
       "                                        1  \\\n",
       "0            00156d23e1eaf8edaad4748a2cbd   \n",
       "1          0013f5df6b77c1c61770a61d5d2687   \n",
       "2           001dd37fa9ec4ec6c502e893b8476   \n",
       "3          0019dcc1de5e66ddabec2c2790fb6f   \n",
       "4          0016c8879d709d7f9eae5ddff5ebcd   \n",
       "...                                   ...   \n",
       "2187129  32a0ea699c2897492580f8e71256d19e   \n",
       "2187134  32a0b0d2f3ba879a9dc483781b152b28   \n",
       "2187136  32a11e7ba9e6dd474755aadbe92bd942   \n",
       "2187145  32a266972a14b2f4a1a39721d9ab36da   \n",
       "2187146  32a3232e355d409cc7f9b0d2b7638326   \n",
       "\n",
       "                                                         2  \\\n",
       "0                                       Chilly soccer game   \n",
       "1                              Stinson Reliant Information   \n",
       "2                  Nightfall Between Bridges, Brooklyn, NY   \n",
       "3                    First amphibious guest of the season!   \n",
       "4                                         Birthday Weekend   \n",
       "...                                                    ...   \n",
       "2187129  Salt Lake County, Utah, Wasatch Hollow, Salt L...   \n",
       "2187134              Barclays Center Arena - 20140419_1220   \n",
       "2187136  Capitol Hill, District of Columbia, Washington...   \n",
       "2187145                                            Bermuda   \n",
       "2187146                                    Hil, her Mom...   \n",
       "\n",
       "                                                         3  \n",
       "0              Dad's a good sport specatator, keeping warm  \n",
       "1        Information panel for the Stinson Reliant mail...  \n",
       "2        A panoramic shot of Manhattan between the Broo...  \n",
       "3        Found this sweetie lounging on a lily pad in m...  \n",
       "4        One thing I learned this weekend was to never ...  \n",
       "...                                                    ...  \n",
       "2187129  Check out the related YouTube video here. - \\n...  \n",
       "2187134  Barclays Center Arena\\nAtlantic Yards\\n6th and...  \n",
       "2187136  Check out the related YouTube video here. - \\n...  \n",
       "2187145  Honeymoon in Bermuda at the Sonesta Beach Reso...  \n",
       "2187146                             ...and Hil's fab shoes  \n",
       "\n",
       "[2165959 rows x 5 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            True\n",
       "1            True\n",
       "2            True\n",
       "3            True\n",
       "4            True\n",
       "            ...  \n",
       "14829391    False\n",
       "14829392    False\n",
       "14829393    False\n",
       "14829394    False\n",
       "14829395    False\n",
       "Name: 1, Length: 14829396, dtype: bool"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.df['1'].isin(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=500x375>,\n",
       " tensor([49406,  2639,   568,   320,   886,  2364,  1711,   527,  1962,   267,\n",
       "          4873,  3616, 49407,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1372291it [03:20, 6831.67it/s]\n"
     ]
    }
   ],
   "source": [
    "def apply_function_to_subfiles(directory):\n",
    "    f = []\n",
    "    for root, dirs, files in tqdm(os.walk(directory)):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            f.append(file_path.split(os.sep)[-1].split(\".\")[0])\n",
    "    return f\n",
    "\n",
    "# Example usage\n",
    "top_directory = \"D:\\data\\YFCC15M\\YFCC15M_extracted\\data\\images\"\n",
    "z = apply_function_to_subfiles(top_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2165959"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = \"D:\\data\\YFCC15M\\YFCC15M_extracted\\data\\images\"\n",
    "os.listdir(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"D:/data/YFCC15M/yfcc15m.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(p, sep='\\t', names=[\"head\", \"caption\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_path(item):\n",
    "    item = item.split(\"/\")[-1]\n",
    "    item = item.replace('_', '')\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df['head'].apply(combine_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://farm8.staticflickr.com/7072/13810813364_33e7e603ae.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['head'].str.len().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from Harmony.other.yfcc100m.yfcc100m.tools import load_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('yfcc100m_dataset.txt') as f:\n",
    "#     for l in tqdm(f.readlines()):\n",
    "#         row = l.strip().split('\\t')\n",
    "#         if int(row[0]) in image_ids:\n",
    "#             uncaptioned.append(int(row[0]))\n",
    "#             if int(row[0]) in clip_ids:\n",
    "#                 title = unquote(row[8]).replace('+', ' ')\n",
    "#                 title = re.sub(cleanhtml, '', title)\n",
    "#                 title = re.sub(cleanurl, '', title)\n",
    "\n",
    "#                 desc = unquote(row[9]).replace('+', ' ')\n",
    "#                 desc = re.sub(cleanhtml, '', desc)\n",
    "#                 desc = re.sub(cleanurl, '', desc)\n",
    "                \n",
    "#                 captioned.append((int(row[0]), title, desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_shards = os.listdir(\"D:/data/YFCC15M/meta\")\n",
    "\n",
    "df_list = []\n",
    "for shard in tqdm(meta_shards):\n",
    "    with gzip.open(Path(\"D:/data/YFCC15M/meta\") / shard, 'rt', encoding='utf-8') as fp:\n",
    "        for line in fp:\n",
    "            df_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import unquote\n",
    "import re\n",
    "cleanhtml = re.compile('<a.*?>|</a>|<b>|</b>|<i>|</i>')\n",
    "cleanurl = re.compile('http\\S+|www\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = []\n",
    "for d in tqdm(df_list):\n",
    "    url = d['downloadurl']\n",
    "    key = d['key']\n",
    "    title = d['title']\n",
    "    desc  = d['description']\n",
    "\n",
    "    title = unquote(title).replace('+', ' ')\n",
    "    title = re.sub(cleanhtml, '', title)\n",
    "    title = re.sub(cleanurl, '', title)\n",
    "\n",
    "    desc = unquote(desc).replace('+', ' ')\n",
    "    desc = re.sub(cleanhtml, '', desc)\n",
    "    desc = re.sub(cleanurl, '', desc)\n",
    "\n",
    "    f.append((url, key, title, desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.names = [\"url\", \"name\", \"title\", \"caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.to_csv(\"yfcc15m.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\Users\\Moham\\Desktop\\KAUST\\results\\t\\checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "zip_directory = \"D:/data/YFCC15M/\"\n",
    "zips = [zip for zip in os.listdir(\"D:\\data\\YFCC15M\") if zip[-4:] == \".zip\"]\n",
    "# zips = zips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import unquote_plus\n",
    "\n",
    "encoded_str = \"This+photo+was+taken+during+our+first+visit+to+Congaree+National+Park+in+June+2011.++It+was+fairly+dry%2C+so+areas+that+are+sometimes+underwater+were+dry.+Congaree+National+Park+preserves+the+largest+tract+of+old+growth+bottomland+hardwood+forest+left+in+the+United+States.+Located+in+South+Carolina%2C+the+22%2C000+acre+national+park+received+that+designation+in+2003+as+the+culmination+of+a+grassroots+campaign+which+had+started+in+1969.+The+lush+trees+growing+in+this+floodplain+forest+are+some+of+the+tallest+in+the+Eastern+U.S.%2C+forming+one+of+the+highest+natural+canopies+remaining+in+the+world.+The+Congaree+River+flows+through+the+park.+15%2C000+acres+%28or+about+70+percent+of+the+park%29+is+designated+wilderness+area.%0A%0AMost+visitors+to+the+park+walk+along+the+Boardwalk+Loop%2C+an+elevated+2.4+mile+walkway+through+the+swampy+environment+that+protects+delicate+fungi+and+plant+life+at+ground+level.%0A%0A%3Ca+href%3D%22http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FCongaree_National_Park%22+rel%3D%22nofollow%22%3Een.wikipedia.org%2Fwiki%2FCongaree_National_Park%3C%2Fa%3E%0A%0A%3Ca+href%3D%22http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FWikipedia%3AText_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License%22+rel%3D%22nofollow%22%3Een.wikipedia.org%2Fwiki%2FWikipedia%3AText_of_Creative_Commons_...%3C%2Fa%3E\"\n",
    "decoded_str = unquote_plus(encoded_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "\n",
    "def list_images_in_zip(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # List all contents and filter for image files\n",
    "        image_files = [item for item in zip_ref.namelist() if item.lower().endswith(('.jpg'))]\n",
    "    return image_files\n",
    "\n",
    "for zip_file in tqdm(zips):\n",
    "    zip_path = os.path.join(zip_directory, zip_file)\n",
    "    image_paths.extend(list_images_in_zip(zip_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_unzipped_size(zip_path):\n",
    "    total_size = 0\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        for info in zip_ref.infolist():\n",
    "            total_size += info.file_size  # Use `file_size` for uncompressed size\n",
    "    return total_size\n",
    "\n",
    "all_size = 0\n",
    "\n",
    "for zip_file in zips:\n",
    "    zip_path = os.path.join(zip_directory, zip_file)\n",
    "    estimated_size = estimate_unzipped_size(zip_path)\n",
    "    all_size += estimated_size\n",
    "    # print(f\"Estimated size for {zip_file} when unzipped: {estimated_size} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_aggregated_size_gb = all_size / (1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_aggregated_size_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, combinations, permutations\n",
    "\n",
    "def power_set_permutations(iterable):\n",
    "    s = list(iterable)\n",
    "    return list(\"_\".join(t) for t in chain.from_iterable(permutations(t) for t in chain.from_iterable(combinations(s, r) for r in range(len(s)+1))))\n",
    "\n",
    "# Test the function\n",
    "print(power_set_permutations(['ibot', 'dino', 'mae']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_ = torch.load(\"ibot_t.pth\")\n",
    "ht  = torch.load(\"harmony_t.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ = torch.load(\"ibot_s.pth\")\n",
    "hs  = torch.load(\"harmony_s.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_same = True\n",
    "\n",
    "# params1 = is_\n",
    "params2 = hs\n",
    "\n",
    "params1 = it_\n",
    "# params2 = ht\n",
    "\n",
    "for key in params1:\n",
    "    if key in params2:\n",
    "        if not torch.equal(params1[key], params2[key]):\n",
    "            print(f\"Difference found in parameter: {key}\")\n",
    "            is_same = False\n",
    "    else:\n",
    "        print(f\"Parameter {key} not found in the second model.\")\n",
    "        is_same = False\n",
    "\n",
    "for key in params2:\n",
    "    if key not in params1:\n",
    "        print(f\"Parameter {key} not found in the first model.\")\n",
    "        is_same = False\n",
    "\n",
    "if is_same:\n",
    "    print(\"The parameters of the two models are exactly the same.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_.values(\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht.values() == it_.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(arch='vit_small', batch_size_per_gpu=1, clip_grad=3.0, data='CC3M:/mnt/d/data/CC3M/cc3m/', dist_url='env://', drop_path_rate=0.1, epochs=100, freeze_last_layer=1, global_crops_scale=(0.4, 1.0), gpu=0, local_crops_number=8, local_crops_scale=(0.05, 0.4), local_rank=0, lr=0.0005, min_lr=1e-06, momentum_teacher=0.996, norm_last_layer=True, num_workers=10, objective='dino', optimizer='adamw', out_dim=65536, output_dir='/mnt/c/Users/Moham/Desktop/KAUST/results', patch_size=16, rank=0, saveckp_freq=20, seed=0, teacher_temp=0.04, use_bn_in_head=False, use_fp16=True, warmup_epochs=10, warmup_teacher_temp=0.04, warmup_teacher_temp_epochs=0, weight_decay=0.04, weight_decay_end=0.4, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(r\"/mnt/c/Users/Moham/Desktop/KAUST/results/checkpoint0000.pth\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.init_distributed_mode(args)\n",
    "# utils.fix_random_seeds(args.seed)\n",
    "\n",
    "# model = Harmony(args=args)\n",
    "\n",
    "model = vits.__dict__[args.arch](patch_size=args.patch_size, num_classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(checkpoint['main_vit'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_key = \"main_vit\"\n",
    "state_dict = torch.load(r\"/mnt/c/Users/Moham/Desktop/KAUST/results/checkpoint0000.pth\", map_location=\"cpu\")\n",
    "\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "# remove `module.` prefix\n",
    "# state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# # remove `backbone.` prefix induced by multicrop wrapper\n",
    "# state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "msg = model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i for i in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint['teacher']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = \"/mnt/c/Users/Moham/Desktop/KAUST/results/checkpoint.pth\"\n",
    "checkpoint_key = \"model\"\n",
    "state_dict = torch.load(pretrained_weights, map_location=\"cpu\")\n",
    "if checkpoint_key is not None and checkpoint_key in state_dict:\n",
    "    print(f\"Take key {checkpoint_key} in provided checkpoint dict\")\n",
    "    state_dict = state_dict[checkpoint_key]\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "print('Pretrained weights found at {} and loaded with msg: {}'.format(pretrained_weights, msg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.discriminative_path.teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.discriminative_path.teacher.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.discriminative_path.teacher.backbone.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as pth_transforms\n",
    "\n",
    "# train_transform = pth_transforms.Compose([\n",
    "#     pth_transforms.RandomResizedCrop(224),\n",
    "#     pth_transforms.RandomHorizontalFlip(),\n",
    "#     pth_transforms.ToTensor(),\n",
    "#     pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "# ])\n",
    "\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(\"/mnt/e/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/train\")\n",
    "\n",
    "data = torch.utils.data.DataLoader(train_data, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.dataset[445245])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dataset[1277363]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 0\n",
    "for i in range(data.__len__()):\n",
    "    r = data.dataset[i]\n",
    "    print(\"class:\", r[1])\n",
    "    print(\"image:\", r[0])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_path = os.path.realpath(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class ImageNet(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split=\"train\", transform=None, **kwargs):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "\n",
    "        if split == \"train\":\n",
    "            self.data = datasets.ImageFolder(root, transform=transform)\n",
    "        elif split == \"val\":\n",
    "            self.images =  os.listdir(self.root)\n",
    "            self.image_paths = [os.path.join(self.root, image) for image in self.images]\n",
    "            self.labels = pd.read_csv(\"./Harmony/data/meta/imagenet_val_labels.csv\")\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.split == \"train\":\n",
    "            return self.data.__len__()\n",
    "        else:\n",
    "            return len(self.images)\n",
    "\n",
    "    def get_image_target(self, idx):\n",
    "        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        image_name = self.images[idx].split('.')[0]\n",
    "        label = self.labels[self.labels['ImageId'] == image_name]['PredictionString'].item()\n",
    "        return image, label\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if self.split == \"train\":\n",
    "            return self.data[idx][0], self.data[idx][1]\n",
    "        \n",
    "        image, target = self.get_image_target(idx)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ImageNet(root=\"/mnt/e/imagenet-object-localization-challenge/ILSVRC/Data/CLS-LOC/val/\", split=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in data:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.vision_transformer import PatchEmbed, Block\n",
    "from vision_transformer import Block\n",
    "\n",
    "from utils import get_2d_sincos_pos_embed\n",
    "\n",
    "class GenerativePath(nn.Module):\n",
    "    def __init__(self, image_encoder, patch_size=16, in_chans=3,\n",
    "                embed_dim=1024, decoder_embed_dim=512, decoder_depth=8, decoder_num_heads=16,\n",
    "                mlp_ratio=4, norm_layer=nn.LayerNorm, norm_pix_loss=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_pix_loss = norm_pix_loss\n",
    "        self.image_encoder = image_encoder\n",
    "        self.patch_embed = image_encoder.patch_embed\n",
    "        num_patches = self.image_encoder.patch_embed.num_patches\n",
    "\n",
    "        self.decoder_embed = nn.Linear(embed_dim, decoder_embed_dim, bias=True)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n",
    "        self.decoder_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, decoder_embed_dim), requires_grad=False)  # fixed sin-cos embedding\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, qk_scale=None, norm_layer=norm_layer)\n",
    "            for i in range(decoder_depth)])\n",
    "\n",
    "        self.decoder_norm = norm_layer(decoder_embed_dim)\n",
    "        self.decoder_pred = nn.Linear(decoder_embed_dim, patch_size**2 * in_chans, bias=True) # decoder to patch\n",
    "\n",
    "    def initialize_deocder_weights(self):\n",
    "        \n",
    "        decoder_pos_embed = get_2d_sincos_pos_embed(self.decoder_pos_embed.shape[-1], int(self.patch_embed.num_patches**.5), cls_token=True)\n",
    "        self.decoder_pos_embed.data.copy_(torch.from_numpy(decoder_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        torch.nn.init.normal_(self.mask_token, std=.02)\n",
    "\n",
    "    def patchify(self, imgs):\n",
    "        \"\"\"\n",
    "        imgs: (N, 3, H, W)\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        \"\"\"\n",
    "        print(self.patch_embed.patch_size)\n",
    "        p = self.patch_embed.patch_size\n",
    "        assert imgs.shape[2] == imgs.shape[3] and imgs.shape[2] % p == 0\n",
    "\n",
    "        h = w = imgs.shape[2] // p\n",
    "        x = imgs.reshape(shape=(imgs.shape[0], 3, h, p, w, p))\n",
    "        x = torch.einsum('nchpwq->nhwpqc', x)\n",
    "        x = x.reshape(shape=(imgs.shape[0], h * w, p**2 * 3))\n",
    "        return x\n",
    "\n",
    "    def unpatchify(self, x):\n",
    "        \"\"\"\n",
    "        x: (N, L, patch_size**2 *3)\n",
    "        imgs: (N, 3, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_embed.patch_size\n",
    "        h = w = int(x.shape[1]**.5)\n",
    "        assert h * w == x.shape[1]\n",
    "        \n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, 3))\n",
    "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], 3, h * p, h * p))\n",
    "        return imgs\n",
    "\n",
    "    def random_masking(self, x, mask_ratio):\n",
    "        \"\"\"\n",
    "        Perform per-sample random masking by per-sample shuffling.\n",
    "        Per-sample shuffling is done by argsort random noise.\n",
    "        x: [N, L, D], sequence\n",
    "        \"\"\"\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)  # noise in [0, 1]\n",
    "        \n",
    "        # sort noise for each sample\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)  # ascend: small is keep, large is remove\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "\n",
    "        # keep the first subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).repeat(1, 1, D))\n",
    "\n",
    "        # generate the binary mask: 0 is keep, 1 is remove\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        # unshuffle to get the binary mask\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "\n",
    "        return x_masked, mask, ids_restore\n",
    "\n",
    "    def forward_encoder(self, x, mask_ratio):\n",
    "        # embed patches\n",
    "        x = self.image_encoder.patch_embed(x)\n",
    "\n",
    "        # add pos embed w/o cls token\n",
    "        x = x + self.image_encoder.pos_embed[:, 1:, :]\n",
    "\n",
    "        # masking: length -> length * mask_ratio\n",
    "        x, mask, ids_restore = self.random_masking(x, mask_ratio)\n",
    "\n",
    "        # append cls token\n",
    "        cls_token = self.image_encoder.cls_token + self.image_encoder.pos_embed[:, :1, :]\n",
    "        cls_tokens = cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.image_encoder.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.image_encoder.norm(x)\n",
    "\n",
    "        return x, mask, ids_restore\n",
    "\n",
    "    def forward_decoder(self, x, ids_restore):\n",
    "        # embed tokens\n",
    "        x = self.decoder_embed(x)\n",
    "\n",
    "        # append mask tokens to sequence\n",
    "        mask_tokens = self.mask_token.repeat(x.shape[0], ids_restore.shape[1] + 1 - x.shape[1], 1)\n",
    "        x_ = torch.cat([x[:, 1:, :], mask_tokens], dim=1)  # no cls token\n",
    "        x_ = torch.gather(x_, dim=1, index=ids_restore.unsqueeze(-1).repeat(1, 1, x.shape[2]))  # unshuffle\n",
    "        x = torch.cat([x[:, :1, :], x_], dim=1)  # append cls token\n",
    "\n",
    "        # add pos embed\n",
    "        x = x + self.decoder_pos_embed\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.decoder_blocks:\n",
    "            x = blk(x)\n",
    "        x = self.decoder_norm(x)\n",
    "\n",
    "        # predictor projection\n",
    "        x = self.decoder_pred(x)\n",
    "\n",
    "        # remove cls token\n",
    "        x = x[:, 1:, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_loss(self, imgs, pred, mask):\n",
    "        \"\"\"\n",
    "        imgs: [N, 3, H, W]\n",
    "        pred: [N, L, p*p*3]\n",
    "        mask: [N, L], 0 is keep, 1 is remove, \n",
    "        \"\"\"\n",
    "        target = self.patchify(imgs)\n",
    "        if self.norm_pix_loss:\n",
    "            mean = target.mean(dim=-1, keepdim=True)\n",
    "            var = target.var(dim=-1, keepdim=True)\n",
    "            target = (target - mean) / (var + 1.e-6)**.5\n",
    "\n",
    "        loss = (pred - target) ** 2\n",
    "        loss = loss.mean(dim=-1)  # [N, L], mean loss per patch\n",
    "\n",
    "        loss = (loss * mask).sum() / mask.sum()  # mean loss on removed patches\n",
    "        return loss\n",
    "\n",
    "    def forward(self, imgs, mask_ratio=0.75):\n",
    "        latent, mask, ids_restore = self.forward_encoder(imgs, mask_ratio)\n",
    "        pred = self.forward_decoder(latent, ids_restore)  # [N, L, p*p*3]\n",
    "        loss = self.forward_loss(imgs, pred, mask)\n",
    "        return loss, pred, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = GenerativePath(image_encoder=model.image_encoder, embed_dim=384).cuda()\n",
    "r = torch.rand([1, 3, 224, 224]).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l, p, m = s(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = CLIP_VITL16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [f for f in os.scandir(\"/mnt/d/data/CC3M/cc3m/\") if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images_in_folders(folders):\n",
    "    images_paths = []\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            if os.path.isfile(os.path.join(folder, filename)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = save_images_in_folders(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from Harmony.data.utils import SimpleTokenizer\n",
    "\n",
    "def save_image_captions_from_folders(folders, root):\n",
    "    images_paths  = []\n",
    "    captions_path = []\n",
    "    for folder in folders:\n",
    "        for filename in os.listdir(folder):\n",
    "            if \".jpg\" in filename or \".png\" in filename:\n",
    "                images_paths.append(root + os.sep + folder.name + os.sep + filename)\n",
    "            elif \".txt\" in filename:\n",
    "                captions_path.append(root + os.sep + folder.name + os.sep + filename)\n",
    "    return images_paths, captions_path\n",
    "\n",
    "class CC3M(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform=None, tokneizer=SimpleTokenizer(), **kwargs):\n",
    "        self.root = root\n",
    "        self.folders =  [f for f in os.scandir(root) if f.is_dir()]\n",
    "        self.images, self.captions = save_image_captions_from_folders(self.folders, self.root)\n",
    "        self.images.sort(), self.captions.sort() # sort to make sure there is correspondence\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokneizer\n",
    "\n",
    "        # self.images = self.images[:10]\n",
    "        # self.captions = self.captions[:10]\n",
    "\n",
    "        assert len(self.captions) == len(self.images)\n",
    "        print(\"Number of images loaded in CC3M are:\", {self.__len__()})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def get_image_caption_pair(self, idx):\n",
    "        print(self.images[idx])\n",
    "        print(self.captions[idx])\n",
    "        image = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        caption_file = open(self.captions[idx])\n",
    "        caption = caption_file.read()\n",
    "        caption_file.close()\n",
    "\n",
    "        return image, caption\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, caption = self.get_image_caption_pair(idx)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.tokenizer:\n",
    "            caption = self.tokenizer(caption)\n",
    "    \n",
    "        return image, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = CC3M(root=\"D:\\data\\CC3M\\cc3m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, t = a[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Open the tar file\n",
    "tar = tarfile.open(r'D:\\data\\CC3M\\cc3m\\00000.tar')\n",
    "\n",
    "# Loop over each member\n",
    "for member in tar.getmembers():\n",
    "    print(member)\n",
    "    # Extract each file as a file object\n",
    "    f = tar.extractfile(member)\n",
    "    if f is not None:\n",
    "        # Read the contents\n",
    "        content = f.read()\n",
    "\n",
    "# Close the tar file\n",
    "tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
